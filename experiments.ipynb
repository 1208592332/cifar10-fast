{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo nvidia-persistenced\n",
    "!sudo nvidia-smi -ac 877,1530"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg = lambda xs, N: 0.0 if N is 0 else np.sum(concat(xs), dtype=np.float)/N\n",
    "        \n",
    "def collect(stats, output):\n",
    "    for k,v in stats.items():\n",
    "        v.append(to_numpy(output[k]))\n",
    "\n",
    "def train_epoch(model, batches, optimizer, lrs, monitors=('loss', 'correct')):\n",
    "    stats = {k:[] for k in monitors}\n",
    "    model.train(True)   \n",
    "    for lr, batch in zip(lrs, batches):  \n",
    "        output = model(batch)\n",
    "        collect(stats, output)\n",
    "        output['loss'].backward()\n",
    "        optimizer.param_groups[0]['lr'] = lr\n",
    "        optimizer.step()\n",
    "        model.zero_grad()\n",
    "    return stats\n",
    "\n",
    "def test_epoch(model, batches, monitors=('loss', 'correct')):\n",
    "    stats = {k:[] for k in monitors}\n",
    "    model.train(False)\n",
    "    for batch in batches:\n",
    "        output = model(batch)\n",
    "        collect(stats, output)\n",
    "    return stats\n",
    "\n",
    "def train(model, lr_schedule, optimizer, train_set, test_set, batch_size=512, loggers=(), num_workers=0):\n",
    "    t = Timer()\n",
    "    train_batches = Batches(train_set, batch_size, shuffle=True, num_workers=num_workers)\n",
    "    test_batches = Batches(test_set, batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    for epoch in range(lr_schedule.knots[-1]):\n",
    "        train_batches.dataset.set_random_choices() \n",
    "        lrs = (lr_schedule(x)/batch_size for x in np.arange(epoch, epoch+1, 1/len(train_batches)))\n",
    "        train_stats, train_time = train_epoch(model, train_batches, optimizer, lrs, ('loss', 'correct')), t()\n",
    "        test_stats, test_time = test_epoch(model, test_batches, ('loss', 'correct')), t()\n",
    "        summary = {\n",
    "           'epoch': epoch+1, 'lr': lr_schedule(epoch+1), \n",
    "            'train time': train_time, 'train loss': avg(train_stats['loss'], len(train_set)), 'train acc': avg(train_stats['correct'], len(train_set)), \n",
    "            'test time': test_time, 'test loss': avg(test_stats['loss'], len(test_set)), 'test acc': avg(test_stats['correct'], len(test_set)),\n",
    "            'total time': t.total_time(), \n",
    "        }\n",
    "        for logger in loggers:\n",
    "            logger.append(summary)    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def res_block(c_in, c_out, stride, **kw):\n",
    "    block = {\n",
    "        'bn1': batch_norm(c_in, **kw),\n",
    "        'relu1': nn.ReLU(True),\n",
    "        'branch': {\n",
    "            'conv1': nn.Conv2d(c_in, c_out, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            'bn2': batch_norm(c_out, **kw),\n",
    "            'relu2': nn.ReLU(True),\n",
    "            'conv2': nn.Conv2d(c_out, c_out, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "        }\n",
    "    }\n",
    "    projection = (stride != 1) or (c_in != c_out)    \n",
    "    if projection:\n",
    "        block['conv3'] = (nn.Conv2d(c_in, c_out, kernel_size=1, stride=stride, padding=0, bias=False), [rel_path('relu1')])\n",
    "    block['add'] =  (Add(), [(rel_path('conv3') if projection else rel_path('relu1')), rel_path('branch', 'conv2')])\n",
    "    return block\n",
    "\n",
    "def DAWN_net(c=64, **kw):\n",
    "    return {\n",
    "        'prep': {'conv': nn.Conv2d(3, c, kernel_size=3, stride=1, padding=1, bias=False)},\n",
    "        'layer1': {\n",
    "            'block0': res_block(c, c, 1, **kw),\n",
    "            'block1': res_block(c, c, 1, **kw),\n",
    "        },\n",
    "        'layer2': {\n",
    "            'block0': res_block(c, 2*c, 2, **kw),\n",
    "            'block1': res_block(2*c, 2*c, 1, **kw),\n",
    "        },\n",
    "        'layer3': {\n",
    "            'block0': res_block(2*c, 4*c, 2, **kw),\n",
    "            'block1': res_block(4*c, 4*c, 1, **kw),\n",
    "        },\n",
    "        'layer4': {\n",
    "            'block0': res_block(4*c, 4*c, 2, **kw),\n",
    "            'block1': res_block(4*c, 4*c, 1, **kw),\n",
    "        },\n",
    "        'classifier': {\n",
    "            'maxpool': nn.MaxPool2d(4),\n",
    "            'avgpool': (nn.AvgPool2d(4), [('layer4', 'block1', 'add')]),\n",
    "            'concat': (Concat(), [rel_path('maxpool'), rel_path('avgpool')]),\n",
    "            'flatten': Flatten(),\n",
    "            'linear': nn.Linear(8*c, 10, bias=True),\n",
    "            'logits': Identity()\n",
    "        }\n",
    "    }\n",
    "\n",
    "losses = {\n",
    "    'loss':  (nn.CrossEntropyLoss(size_average=False), [('classifier','logits'), ('target',)]),\n",
    "    'correct': (Correct(), [('classifier','logits'), ('target',)]),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Preprocessing training data\n",
      "Finished in 4.1 seconds\n",
      "Preprocessing test data\n",
      "Finished in 0.12 seconds\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = './data'\n",
    "\n",
    "train_set_raw = torchvision.datasets.CIFAR10(root=DATA_DIR, train=True, download=True)\n",
    "test_set_raw = torchvision.datasets.CIFAR10(root=DATA_DIR, train=False, download=True)\n",
    "t = Timer()\n",
    "print('Preprocessing training data')\n",
    "train_set = list(zip(transpose(normalise(pad(train_set_raw.train_data, 4))), train_set_raw.train_labels))\n",
    "print(f'Finished in {t():.2} seconds')\n",
    "print('Preprocessing test data')\n",
    "test_set = list(zip(transpose(normalise(test_set_raw.test_data)), test_set_raw.test_labels))\n",
    "print(f'Finished in {t():.2} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Post 1: Baseline](https://www.myrtle.ai/2018/09/24/how_to_train_your_resnet_1/) - bulk random choices (300s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_schedule = PiecewiseLinear([0, 15, 30, 35], [0, 0.1, 0.005, 0])\n",
    "batch_size = 128\n",
    "\n",
    "n = DAWN_net()\n",
    "display(DotGraph(n))\n",
    "model = TorchGraph(union(n, losses)).to(device)\n",
    "#convert all children including batch norms to half precision (triggering slow codepath!)\n",
    "for v in model.children(): \n",
    "    v.half()\n",
    "opt = nesterov(trainable_params(model), momentum=0.9, weight_decay=5e-4*batch_size)\n",
    "train_set_x = Transform(train_set, [Crop(32, 32), FlipLR()])\n",
    "summary=train(model, lr_schedule, opt, train_set_x, test_set, \n",
    "              batch_size=batch_size, loggers=(TableLogger(),), \n",
    "              num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Post 1: Baseline](https://www.myrtle.ai/2018/09/24/how_to_train_your_resnet_1/) - final (297s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_schedule = PiecewiseLinear([0, 15, 30, 35], [0, 0.1, 0.005, 0])\n",
    "batch_size = 128\n",
    "\n",
    "n = DAWN_net()\n",
    "display(DotGraph(n))\n",
    "model = TorchGraph(union(n, losses)).to(device)\n",
    "#convert all children including batch norms to half precision (triggering slow codepath!)\n",
    "for v in model.children(): \n",
    "    v.half()\n",
    "opt = nesterov(trainable_params(model), momentum=0.9, weight_decay=5e-4*batch_size)\n",
    "train_set_x = Transform(train_set, [Crop(32, 32), FlipLR()])\n",
    "summary=train(model, lr_schedule, opt, train_set_x, test_set, \n",
    "              batch_size=batch_size, loggers=(TableLogger(),), \n",
    "              num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Post 2: Mini-batches](https://www.myrtle.ai/2018/09/24/how_to_train_your_resnet_2/) - batch size=512 ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_schedule = PiecewiseLinear([0, 15, 30, 35], [0, 0.44, 0.005, 0])\n",
    "batch_size = 512\n",
    "\n",
    "n = DAWN_net()\n",
    "display(DotGraph(n))\n",
    "model = TorchGraph(union(n, losses)).to(device)\n",
    "#convert all children including batch norms to half precision (triggering slow codepath!)\n",
    "for v in model.children(): \n",
    "    v.half()\n",
    "opt = nesterov(trainable_params(model), momentum=0.9, weight_decay=5e-4*batch_size)\n",
    "train_set_x = Transform(train_set, [Crop(32, 32), FlipLR()])\n",
    "summary=train(model, lr_schedule, opt, train_set_x, test_set, \n",
    "              batch_size=batch_size, loggers=(TableLogger(),), \n",
    "              num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Post 3: Regularisation](https://www.myrtle.ai/2018/09/24/how_to_train_your_resnet_3/) - speed up batch norms ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_schedule = PiecewiseLinear([0, 15, 30, 35], [0, 0.44, 0.005, 0])\n",
    "batch_size = 512\n",
    "\n",
    "n = DAWN_net()\n",
    "display(DotGraph(n))\n",
    "model = TorchGraph(union(n, losses)).to(device).half()\n",
    "opt = nesterov(trainable_params(model), momentum=0.9, weight_decay=5e-4*batch_size)\n",
    "train_set_x = Transform(train_set, [Crop(32, 32), FlipLR()])\n",
    "summary=train(model, lr_schedule, opt, train_set_x, test_set, \n",
    "              batch_size=batch_size, loggers=(TableLogger(),), \n",
    "              num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Post 3: Regularisation](https://www.myrtle.ai/2018/09/24/how_to_train_your_resnet_3/) - final ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_schedule = PiecewiseLinear([0, 8, 30], [0, 0.40, 0])\n",
    "batch_size = 512\n",
    "\n",
    "n = DAWN_net()\n",
    "display(DotGraph(n))\n",
    "model = TorchGraph(union(n, losses)).to(device).half()\n",
    "opt = nesterov(trainable_params(model), momentum=0.9, weight_decay=5e-4*batch_size)\n",
    "train_set_x = Transform(train_set, [Crop(32, 32), FlipLR(), Cutout(8,8)])\n",
    "summary=train(model, lr_schedule, opt, train_set_x, test_set, \n",
    "              batch_size=batch_size, loggers=(TableLogger(),), \n",
    "              num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
