{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set GPU clocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applications clocks set to \"(MEM 877, SM 1530)\" for GPU 00000000:00:1E.0\n",
      "All done.\n"
     ]
    }
   ],
   "source": [
    "!sudo nvidia-persistenced\n",
    "!sudo nvidia-smi -ac 877,1530"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg = lambda xs, N: 0.0 if N is 0 else np.sum(concat(xs), dtype=np.float)/N\n",
    "        \n",
    "def collect(stats, output):\n",
    "    for k,v in stats.items():\n",
    "        v.append(to_numpy(output[k]))\n",
    "\n",
    "def train_epoch(model, batches, optimizer, lrs, monitors=('loss', 'correct')):\n",
    "    stats = {k:[] for k in monitors}\n",
    "    model.train(True)   \n",
    "    for lr, batch in zip(lrs, batches):  \n",
    "        output = model(batch)\n",
    "        collect(stats, output)\n",
    "        output['loss'].backward()\n",
    "        optimizer.param_groups[0]['lr'] = lr\n",
    "        optimizer.step()\n",
    "        model.zero_grad()\n",
    "    return stats\n",
    "\n",
    "def test_epoch(model, batches, monitors=('loss', 'correct')):\n",
    "    stats = {k:[] for k in monitors}\n",
    "    model.train(False)\n",
    "    for batch in batches:\n",
    "        output = model(batch)\n",
    "        collect(stats, output)\n",
    "    return stats\n",
    "\n",
    "def train(model, lr_schedule, optimizer, train_set, test_set, batch_size=512, loggers=(TableLogger(),)):\n",
    "    t = Timer()\n",
    "    train_batches = Batches(train_set, batch_size, shuffle=True, num_workers=0)\n",
    "    test_batches = Batches(test_set, batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "    for epoch in range(lr_schedule.knots[-1]):\n",
    "        train_batches.dataset.set_random_choices() \n",
    "        lrs = (lr_schedule(x)/batch_size for x in np.arange(epoch, epoch+1, 1/len(train_batches)))\n",
    "        train_stats, train_time = train_epoch(model, train_batches, optimizer, lrs, ('loss', 'correct')), t()\n",
    "        test_stats, test_time = test_epoch(model, test_batches, ('loss', 'correct')), t()\n",
    "        summary = {\n",
    "           'epoch': epoch+1, 'lr': lr_schedule(epoch+1), \n",
    "            'train time': train_time, 'train loss': avg(train_stats['loss'], len(train_set)), 'train acc': avg(train_stats['correct'], len(train_set)), \n",
    "            'test time': test_time, 'test loss': avg(test_stats['loss'], len(test_set)), 'test acc': avg(test_stats['correct'], len(test_set)),\n",
    "            'total time': t.total_time(), \n",
    "        }\n",
    "        for logger in loggers:\n",
    "            logger.append(summary)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_bn(c_in, c_out, **kw):\n",
    "    return {'conv': nn.Conv2d(c_in, c_out, kernel_size=3, stride=1, padding=1, bias=False), 'bn': batch_norm(c_out, **kw), 'relu': nn.ReLU(True)}\n",
    "\n",
    "def residual(c, **kw):\n",
    "    return {\n",
    "        'in': Identity(),\n",
    "        'res1': conv_bn(c, c, **kw),\n",
    "        'res2': conv_bn(c, c, **kw),\n",
    "        'add': (Add(), [rel_path('in'), rel_path('res2', 'relu')]),\n",
    "    }\n",
    "\n",
    "def basic_network(channels, weight,  pool, **kw):\n",
    "    return {\n",
    "        'prep': conv_bn(3, channels['prep'], **kw),\n",
    "        'layer1': dict(conv_bn(channels['prep'], channels['layer1'], **kw), pool=pool),\n",
    "        'layer2': dict(conv_bn(channels['layer1'], channels['layer2'], **kw), pool=pool),\n",
    "        'layer3': dict(conv_bn(channels['layer2'], channels['layer3'], **kw), pool=pool),\n",
    "        'classifier': {\n",
    "            'pool': nn.AdaptiveMaxPool2d(1),\n",
    "            'flatten': Flatten(),\n",
    "            'linear': nn.Linear(c['layer3'], 10, bias=False),\n",
    "            'logits': Mul(weight),\n",
    "        }\n",
    "    }\n",
    "\n",
    "def network(channels, weight=0.125, pool=nn.MaxPool2d(2), extra_layers=(), res_layers=('layer1', 'layer3'), **kw):\n",
    "    n = basic_network(channels, weight, pool, **kw)\n",
    "    for layer in res_layers:\n",
    "        n[layer]['residual'] = residual(channels[layer], **kw)\n",
    "    for layer in extra_layers:\n",
    "        n[layer]['extra'] = conv_bn(channels[layer], channels[layer], **kw)       \n",
    "    return n\n",
    "\n",
    "losses = {\n",
    "    'loss':  (nn.CrossEntropyLoss(size_average=False), [('classifier','logits'), ('target',)]),\n",
    "    'correct': (Correct(), [('classifier','logits'), ('target',)]),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n",
      "Files already downloaded and verified\n",
      "Preprocessing training data\n",
      "Finished in 4.4 seconds\n",
      "Preprocessing test data\n",
      "Finished in 0.12 seconds\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = './data'\n",
    "\n",
    "train_set_raw = torchvision.datasets.CIFAR10(root=DATA_DIR, train=True, download=True)\n",
    "test_set_raw = torchvision.datasets.CIFAR10(root=DATA_DIR, train=False, download=True)\n",
    "t = Timer()\n",
    "print('Preprocessing training data')\n",
    "train_set = list(zip(transpose(normalise(pad(train_set_raw.train_data, 4))), train_set_raw.train_labels))\n",
    "print(f'Finished in {t():.2} seconds')\n",
    "print('Preprocessing test data')\n",
    "test_set = list(zip(transpose(normalise(test_set_raw.test_data)), test_set_raw.test_labels))\n",
    "print(f'Finished in {t():.2} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warming up cudnn on a batch of random inputs\n",
      "Finished in 1.18 seconds\n",
      "Starting training at 2018-10-01 12:45:33\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86c29f3de35141f7a8361a2496b3dae0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training at 2018-10-01 12:47:01\n"
     ]
    }
   ],
   "source": [
    "lr_schedule = piecewise_linear([0,5,24], [0, 0.4, 0])\n",
    "batch_size = 512\n",
    "cutout=8\n",
    "\n",
    "net = network({'prep': 64, 'layer1': 128, 'layer2': 256, 'layer3': 512})\n",
    "display(DotGraph(net))\n",
    "model = TorchGraph(net).to(device).half()\n",
    "\n",
    "t = Timer()\n",
    "print('Warming up cudnn on a batch of random inputs')\n",
    "warmup_cudnn(model, batch_size)\n",
    "print(f'Finished in {t():.3} seconds')\n",
    "\n",
    "opt = nesterov(trainable_params(model), momentum=0.9, weight_decay=5e-4*batch_size)\n",
    "train(model, lr_schedule, opt, Transform(train_set, [Crop(32, 32), FlipLR(), Cutout(cutout, cutout)]), test_set, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
